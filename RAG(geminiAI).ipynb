{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a99aba2-3e98-4e69-9725-25695f191c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install google-generativeai langchain langchain_google_genai langchain_text_splitters pypdf annoy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e7320b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install langchain_community\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14cb26be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pdfplumber\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42fa6e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Annoy\n",
    "from langchain.schema.document import Document\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddd22cdd-813e-4833-b74b-b268e7cbfbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad84f674",
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fdc48c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "\n",
    "data_path = \"data\"\n",
    "\n",
    "def load_pdf():\n",
    "    documents = []\n",
    "    for pdf_file in os.listdir(data_path):\n",
    "        if pdf_file.endswith(\".pdf\"):\n",
    "            with pdfplumber.open(os.path.join(data_path, pdf_file)) as pdf:\n",
    "                text = \"\\n\".join(page.extract_text() for page in pdf.pages if page.extract_text())\n",
    "                documents.append(Document(page_content=text))\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3b6813b-23be-4140-afd0-c526ed251d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.DS_Store', 'requirements.txt', 'anaconda_projects', '.gitignore', '.env', '.virtual_documents', '.ipynb_checkpoints', 'RAG(geminiAI).ipynb', 'demo.py', 'data', 'RAG(geminiAI)-Copy1.ipynb', 'Untitled.ipynb 10-30-05-375\\u202fPM-Copy1.ipynb']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed210ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"API Key: {api_key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35f653aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = load_pdf()\n",
    "# documents[0],len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0cae850e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 1:\n",
      "THE KITE\n",
      "RUNNER\n",
      "by KHALED HOSSEINI\n",
      "Published 2003\n",
      "Afghan Mellat Online Library\n",
      "www.afghan-­‐mellat.org.uk\n",
      "_December 2001_\n",
      "I became what I am today at the age of twelve, on a frigid overcast day in the\n",
      "winter of 1975. I remember the precise moment, crouching behind a crumbling\n",
      "mud wall, peeking into the alley near the frozen creek. That was a long time ago,\n",
      "but it's wrong what they say about the past, I've learned, about how you can bury\n",
      "it. Because the past claws its way out. Looking back now, I...\n",
      "\n",
      "\n",
      "Document 2:\n",
      "MONOPOLY\n",
      "Property Trading Game from Parker Brothers\"\n",
      "AGES 8+\n",
      "2 to 8 Players\n",
      "Contents: Gameboard, 3 dice, tokens, 32 houses, I2 hotels, Chance\n",
      "and Community Chest cards, Title Deed cards, play money and a Banker's tray.\n",
      "Now there's a faster way to play MONOPOLY. Choose to play by\n",
      "the classic rules for buying, renting and selling properties or use the\n",
      "Speed Die to get into the action faster. If you've never played the classic\n",
      "MONOPOLY game, refer to the Classic Rules beginning on the next page.\n",
      "If...\n",
      "\n",
      "\n",
      "Document 3:\n",
      "Attention Is All You Need\n",
      "AshishVaswani∗ NoamShazeer∗ NikiParmar∗ JakobUszkoreit∗\n",
      "GoogleBrain GoogleBrain GoogleResearch GoogleResearch\n",
      "avaswani@google.com noam@google.com nikip@google.com usz@google.com\n",
      "LlionJones∗ AidanN.Gomez∗ † ŁukaszKaiser∗\n",
      "GoogleResearch UniversityofToronto GoogleBrain\n",
      "llion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com\n",
      "IlliaPolosukhin∗ ‡\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "Thedominantsequencetransductionmodelsarebasedoncomplexrecurrentor\n",
      "convolutionalneuralnetwo...\n",
      "\n",
      "\n",
      "Document 4:\n",
      "Clustering Data Streams\n",
      "Sudipto Guha ∗ Nina Mishra † Rajeev Motwani ‡ Liadan O’Callaghan §\n",
      "Abstract large to (cid:12)t in main memory and are typically stored\n",
      "in secondary storage devices, making access, particu-\n",
      "We study clustering under the data stream model larly random access, very expensive. Data stream al-\n",
      "of computation where: given a sequence of points, the gorithms access the input only via linear scans with-\n",
      "objectiveistomaintainaconsistentlygoodclusteringof out random access and only ...\n",
      "\n",
      "\n",
      "Document 5:\n",
      "Fine-tuning and Utilization Methods of Domain-specific LLMs\n",
      "Cheonsu Jeong1\n",
      "Dr. Jeong is Principal Consultant & the Technical Leader for AI Automation at SAMSUNG SDS\n",
      "Abstract\n",
      "Recent releases of pre-trained Large Language Models (LLMs) have gained considerable traction,\n",
      "yet research on fine-tuning and employing domain-specific LLMs remains scarce. This study\n",
      "investigates approaches for fine-tuning and leveraging domain-specific LLMs, highlighting trends\n",
      "in LLMs, foundational models, and methods fo...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check extracted text\n",
    "for i, doc in enumerate(documents[:5]):  # Print first 5 docs\n",
    "    print(f\"\\nDocument {i+1}:\\n{doc.page_content[:500]}...\\n\")  # Print first 500 chars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0771ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=80,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    return text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2424eb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='THE KITE\n",
      "RUNNER\n",
      "by KHALED HOSSEINI\n",
      "Published 2003\n",
      "Afghan Mellat Online Library\n",
      "www.afghan-­‐mellat.org.uk\n",
      "_December 2001_\n",
      "I became what I am today at the age of twelve, on a frigid overcast day in the\n",
      "winter of 1975. I remember the precise moment, crouching behind a crumbling\n",
      "mud wall, peeking into the alley near the frozen creek. That was a long time ago,\n",
      "but it's wrong what they say about the past, I've learned, about how you can bury\n",
      "it. Because the past claws its way out. Looking back now, I realize I have been\n",
      "peeking into that deserted alley for the last twenty-­‐six years.\n",
      "One day last summer, my friend Rahim Khan called from Pakistan. He\n",
      "asked me to come see him. Standing in the kitchen with the receiver to my ear, I'\n"
     ]
    }
   ],
   "source": [
    "chunks = split_documents(documents)\n",
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "263a6953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"THE KITE\\nRUNNER\\nby KHALED HOSSEINI\\nPublished 2003\\nAfghan Mellat Online Library\\nwww.afghan-\\xad‐mellat.org.uk\\n_December 2001_\\nI became what I am today at the age of twelve, on a frigid overcast day in the\\nwinter of 1975. I remember the precise moment, crouching behind a crumbling\\nmud wall, peeking into the alley near the frozen creek. That was a long time ago,\\nbut it's wrong what they say about the past, I've learned, about how you can bury\\nit. Because the past claws its way out. Looking back now, I realize I have been\\npeeking into that deserted alley for the last twenty-\\xad‐six years.\\nOne day last summer, my friend Rahim Khan called from Pakistan. He\\nasked me to come see him. Standing in the kitchen with the receiver to my ear, I\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract plain text from the Document objects\n",
    "chunk_texts = [chunk.page_content for chunk in chunks]\n",
    "chunk_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dab2e049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annoy vector store successfully created!\n"
     ]
    }
   ],
   "source": [
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\",\n",
    "    google_api_key=api_key\n",
    ")\n",
    "annoy_store = Annoy.from_texts(chunk_texts, embeddings)\n",
    "print(\"Annoy vector store successfully created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "80b12bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea97399-e761-495d-8c5d-f3f429ef74d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "007f5fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Ask a question about the PDF (or type 'exit' to quit):  How does the self-attention mechanism in the Transformer model improve upon traditional sequence-to-sequence architectures like RNNs and LSTMs?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Most Relevant Text from PDF:\n",
      " to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\n",
      "describedinsection3.2.\n",
      "Self-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions\n",
      "ofasinglesequenceinordertocomputearepresentationofthesequence. Self-attentionhasbeen\n",
      "usedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization,\n",
      "textualentailmentandlearningtask-independentsentencerepresentations[4,22,23,19].\n",
      "End-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-\n",
      "alignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand\n",
      "languagemodelingtasks[28].\n",
      "To the best of our knowledge, however, the Transformer is the first transduction model relying 3.2.3 ApplicationsofAttentioninourModel\n",
      "TheTransformerusesmulti-headattentioninthreedifferentways:\n",
      "• In\"encoder-decoderattention\"layers,thequeriescomefromthepreviousdecoderlayer,\n",
      "andthememorykeysandvaluescomefromtheoutputoftheencoder. Thisallowsevery\n",
      "positioninthedecodertoattendoverallpositionsintheinputsequence. Thismimicsthe\n",
      "typical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n",
      "[31,2,8].\n",
      "• Theencodercontainsself-attentionlayers. Inaself-attentionlayerallofthekeys,values\n",
      "andqueriescomefromthesameplace,inthiscase,theoutputofthepreviouslayerinthe\n",
      "encoder. Eachpositionintheencodercanattendtoallpositionsinthepreviouslayerofthe\n",
      "encoder.\n",
      "• Similarly,self-attentionlayersinthedecoderalloweachpositioninthedecodertoattendto entirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-\n",
      "alignedRNNsorconvolution. Inthefollowingsections,wewilldescribetheTransformer,motivate\n",
      "self-attentionanddiscussitsadvantagesovermodelssuchas[14,15]and[8].\n",
      "3 ModelArchitecture\n",
      "Mostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,29].\n",
      "Here, the encoder maps an input sequence of symbol representations (x ,...,x ) to a sequence\n",
      "1 n\n",
      "of continuous representations z = (z ,...,z ). Given z, the decoder then generates an output\n",
      "1 n\n",
      "sequence(y ,...,y )ofsymbolsoneelementatatime. Ateachstepthemodelisauto-regressive\n",
      "1 m\n",
      "[9],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext. big 6 1024 4096 16 0.3 300K 4.33 26.4 213\n",
      "InTable3rows(B),weobservethatreducingtheattentionkeysized hurtsmodelquality. This\n",
      "k\n",
      "suggests that determining compatibility is not easy and that a more sophisticated compatibility\n",
      "functionthandotproductmaybebeneficial. Wefurtherobserveinrows(C)and(D)that,asexpected,\n",
      "biggermodelsarebetter,anddropoutisveryhelpfulinavoidingover-fitting.Inrow(E)wereplaceour\n",
      "sinusoidalpositionalencodingwithlearnedpositionalembeddings[8],andobservenearlyidentical\n",
      "resultstothebasemodel.\n",
      "7 Conclusion\n",
      "Inthiswork,wepresentedtheTransformer,thefirstsequencetransductionmodelbasedentirelyon\n",
      "attention,replacingtherecurrentlayersmostcommonlyusedinencoder-decoderarchitectureswith\n",
      "multi-headedself-attention. Attention Is All You Need\n",
      "AshishVaswani∗ NoamShazeer∗ NikiParmar∗ JakobUszkoreit∗\n",
      "GoogleBrain GoogleBrain GoogleResearch GoogleResearch\n",
      "avaswani@google.com noam@google.com nikip@google.com usz@google.com\n",
      "LlionJones∗ AidanN.Gomez∗ † ŁukaszKaiser∗\n",
      "GoogleResearch UniversityofToronto GoogleBrain\n",
      "llion@google.com aidan@cs.toronto.edu lukaszkaiser@google.com\n",
      "IlliaPolosukhin∗ ‡\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "Thedominantsequencetransductionmodelsarebasedoncomplexrecurrentor\n",
      "convolutionalneuralnetworksthatincludeanencoderandadecoder. Thebest\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "basedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions\n"
     ]
    },
    {
     "ename": "NotFound",
     "evalue": "404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFound\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 33\u001b[0m\n\u001b[1;32m     27\u001b[0m model \u001b[38;5;241m=\u001b[39m genai\u001b[38;5;241m.\u001b[39mGenerativeModel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemini-pro\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m generation_config \u001b[38;5;241m=\u001b[39m genai\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mGenerationConfig(\n\u001b[1;32m     29\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m,\n\u001b[1;32m     30\u001b[0m     max_output_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m,\n\u001b[1;32m     31\u001b[0m     top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m,\n\u001b[1;32m     32\u001b[0m )\n\u001b[0;32m---> 33\u001b[0m response \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate_content(full_prompt)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(response, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcandidates\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcandidates:\n\u001b[1;32m     36\u001b[0m     best_candidate \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mcandidates[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Assuming first is the best\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/google/generativeai/generative_models.py:331\u001b[0m, in \u001b[0;36mGenerativeModel.generate_content\u001b[0;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_iterator(iterator)\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mgenerate_content(\n\u001b[1;32m    332\u001b[0m             request,\n\u001b[1;32m    333\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest_options,\n\u001b[1;32m    334\u001b[0m         )\n\u001b[1;32m    335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_response(response)\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m google\u001b[38;5;241m.\u001b[39mapi_core\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mInvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:830\u001b[0m, in \u001b[0;36mGenerativeServiceClient.generate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m--> 830\u001b[0m response \u001b[38;5;241m=\u001b[39m rpc(\n\u001b[1;32m    831\u001b[0m     request,\n\u001b[1;32m    832\u001b[0m     retry\u001b[38;5;241m=\u001b[39mretry,\n\u001b[1;32m    833\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    834\u001b[0m     metadata\u001b[38;5;241m=\u001b[39mmetadata,\n\u001b[1;32m    835\u001b[0m )\n\u001b[1;32m    837\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/google/api_core/gapic_v1/method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    292\u001b[0m )\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retry_target(\n\u001b[1;32m    294\u001b[0m     target,\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predicate,\n\u001b[1;32m    296\u001b[0m     sleep_generator,\n\u001b[1;32m    297\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout,\n\u001b[1;32m    298\u001b[0m     on_error\u001b[38;5;241m=\u001b[39mon_error,\n\u001b[1;32m    299\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:153\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     _retry_error_helper(\n\u001b[1;32m    154\u001b[0m         exc,\n\u001b[1;32m    155\u001b[0m         deadline,\n\u001b[1;32m    156\u001b[0m         sleep,\n\u001b[1;32m    157\u001b[0m         error_list,\n\u001b[1;32m    158\u001b[0m         predicate,\n\u001b[1;32m    159\u001b[0m         on_error,\n\u001b[1;32m    160\u001b[0m         exception_factory,\n\u001b[1;32m    161\u001b[0m         timeout,\n\u001b[1;32m    162\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(sleep)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/google/api_core/retry/retry_base.py:212\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[1;32m    207\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[1;32m    208\u001b[0m         error_list,\n\u001b[1;32m    209\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[1;32m    210\u001b[0m         original_timeout,\n\u001b[1;32m    211\u001b[0m     )\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     on_error_fn(exc)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m         result \u001b[38;5;241m=\u001b[39m target()\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[1;32m    146\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/google/api_core/timeout.py:120\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# Avoid setting negative timeout\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout \u001b[38;5;241m-\u001b[39m time_since_first_attempt)\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/google/api_core/grpc_helpers.py:78\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mNotFound\u001b[0m: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods."
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    query = input(\"\\nAsk a question about the PDF (or type 'exit' to quit): \")\n",
    "    if query.lower() == \"exit\":\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "\n",
    "    query_embedding = embeddings.embed_query(query)\n",
    "    similar_docs = annoy_store.similarity_search_by_vector(query_embedding, k=5)\n",
    "\n",
    "    if similar_docs:\n",
    "        pdf_context = \" \".join(doc.page_content for doc in similar_docs)\n",
    "    else:\n",
    "        pdf_context = \"No relevant context found.\"\n",
    "        \n",
    "    print(\"\\n Most Relevant Text from PDF:\\n\", pdf_context)\n",
    "\n",
    "    full_prompt = f\"\"\"\n",
    "    You are an AI assistant. Answer the user's question based on the extracted PDF context.\n",
    "\n",
    "    **Question:** {query}\n",
    "\n",
    "    **PDF Context:** {pdf_context}\n",
    "\n",
    "    Provide a detailed and informative response. Explain thoroughly, provide examples, and ensure clarity. If needed, break your response into sections for better readability.\n",
    "    \"\"\"\n",
    "\n",
    "    model = genai.GenerativeModel(\"gemini-pro\")\n",
    "    generation_config = genai.types.GenerationConfig(\n",
    "        temperature=0.7,\n",
    "        max_output_tokens=1024,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    response = model.generate_content(full_prompt)\n",
    "\n",
    "    if response and hasattr(response, \"candidates\") and response.candidates:\n",
    "        best_candidate = response.candidates[0]  # Assuming first is the best\n",
    "        full_text = \" \".join(part.text for part in best_candidate.content.parts)  # Combine all parts\n",
    "        display(Markdown(full_text))\n",
    "    else:\n",
    "        print(\"\\n No valid response received from Gemini AI.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9584158",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
